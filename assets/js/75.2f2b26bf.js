(window.webpackJsonp=window.webpackJsonp||[]).push([[75],{693:function(s,a,r){"use strict";r.r(a);var e=r(12),t=Object(e.a)({},(function(){var s=this,a=s.$createElement,r=s._self._c||a;return r("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[r("h1",{attrs:{id:"scrapy-redis分布式爬虫"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#scrapy-redis分布式爬虫"}},[s._v("#")]),s._v(" Scrapy-Redis分布式爬虫")]),s._v(" "),r("h2",{attrs:{id:"scrapy-redis分布式爬虫组件"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#scrapy-redis分布式爬虫组件"}},[s._v("#")]),s._v(" Scrapy-Redis分布式爬虫组件")]),s._v(" "),r("p",[s._v("Scrapy是一个框架，他本身是不支持分布式的。如果我们想要做分布式的爬虫，就需要借助一个组件叫做Scrapy-Redis，这个组件正是利用了Redis可以分布式的功能，集成到Scrapy框架中，使得爬虫可以进行分布式。可以充分的利用资源（多个ip、更多带宽、同步爬取）来提高爬虫的爬行效率。")]),s._v(" "),r("h2",{attrs:{id:"分布式爬虫的优点"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#分布式爬虫的优点"}},[s._v("#")]),s._v(" 分布式爬虫的优点")]),s._v(" "),r("ul",[r("li",[s._v("可以充分利用多台机器的带宽")]),s._v(" "),r("li",[s._v("可以充分利用多台机器的ip地址")]),s._v(" "),r("li",[s._v("多台机器做，爬取效率更高")])]),s._v(" "),r("h2",{attrs:{id:"分布式爬虫必须要解决的问题"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#分布式爬虫必须要解决的问题"}},[s._v("#")]),s._v(" 分布式爬虫必须要解决的问题")]),s._v(" "),r("ul",[r("li",[s._v("分布式爬虫是好几台机器在同时运行，如何保证不同的机器爬取页面的时候不会出现重复爬取的问题")]),s._v(" "),r("li",[s._v("分布式爬虫在不同的机器上运行，在把数据爬完后如何保证保存在同一个地方")])]),s._v(" "),r("h2",{attrs:{id:"安装"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#安装"}},[s._v("#")]),s._v(" 安装")]),s._v(" "),r("p",[s._v("通过 "),r("code",[s._v("pip install scrapy-redis")]),s._v(" 即可安装")]),s._v(" "),r("h2",{attrs:{id:"将scrapy框架开发的爬虫项目改为分布式爬虫项目"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#将scrapy框架开发的爬虫项目改为分布式爬虫项目"}},[s._v("#")]),s._v(" 将scrapy框架开发的爬虫项目改为分布式爬虫项目")]),s._v(" "),r("ol",[r("li",[r("p",[s._v("将爬虫的类从"),r("code",[s._v("scrapy.Spider")]),s._v("变成"),r("code",[s._v("scrapy_redis.spiders.RedisSpider")]),s._v("，或者是从"),r("code",[s._v("scrapy.CrawlSpider")]),s._v("变成"),r("code",[s._v("scrapy_redis.spiders.RedisCrawlSpider")])])]),s._v(" "),r("li",[r("p",[s._v("将爬虫中的"),r("code",[s._v("start_urls")]),s._v("删掉，增加一个 "),r("code",[s._v('redis_key="xxx"')]),s._v(" 。这个"),r("code",[s._v("redis_key")]),s._v("是为了以后在redis中控制爬虫启动的。爬虫的第一个url，就是在redis中通过这个发送出去的。")])]),s._v(" "),r("li",[r("p",[s._v("在配置文件中增加如下配置")]),s._v(" "),r("div",{staticClass:"language-python line-numbers-mode"},[r("pre",{pre:!0,attrs:{class:"language-python"}},[r("code",[s._v("     "),r("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# Scrapy-Redis相关配置")]),s._v("\n\n     "),r("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 确保request存储到redis中")]),s._v("\n     SCHEDULER "),r("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),r("span",{pre:!0,attrs:{class:"token string"}},[s._v('"scrapy_redis.scheduler.Scheduler"')]),s._v("\n\n     "),r("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 确保所有爬虫共享相同的去重指纹")]),s._v("\n     DUPEFILTER_CLASS "),r("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),r("span",{pre:!0,attrs:{class:"token string"}},[s._v('"scrapy_redis.dupefilter.RFPDupeFilter"')]),s._v("\n\n     "),r("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 设置redis为item pipeline")]),s._v("\n     ITEM_PIPELINES "),r("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),r("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n         "),r("span",{pre:!0,attrs:{class:"token string"}},[s._v("'scrapy_redis.pipelines.RedisPipeline'")]),r("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" "),r("span",{pre:!0,attrs:{class:"token number"}},[s._v("300")]),s._v("\n     "),r("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n\n     "),r("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 在redis中保持scrapy-redis用到的队列，不会清理redis中的队列，从而可以实现暂停和恢复的功能")]),s._v("\n     SCHEDULER_PERSIST "),r("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),r("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),s._v("\n\n     "),r("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 设置连接redis信息")]),s._v("\n     REDIS_HOST "),r("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),r("span",{pre:!0,attrs:{class:"token string"}},[s._v("'127.0.0.1'")]),s._v("\n     REDIS_PORT "),r("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),r("span",{pre:!0,attrs:{class:"token number"}},[s._v("6379")]),s._v("\n")])]),s._v(" "),r("div",{staticClass:"line-numbers-wrapper"},[r("span",{staticClass:"line-number"},[s._v("1")]),r("br"),r("span",{staticClass:"line-number"},[s._v("2")]),r("br"),r("span",{staticClass:"line-number"},[s._v("3")]),r("br"),r("span",{staticClass:"line-number"},[s._v("4")]),r("br"),r("span",{staticClass:"line-number"},[s._v("5")]),r("br"),r("span",{staticClass:"line-number"},[s._v("6")]),r("br"),r("span",{staticClass:"line-number"},[s._v("7")]),r("br"),r("span",{staticClass:"line-number"},[s._v("8")]),r("br"),r("span",{staticClass:"line-number"},[s._v("9")]),r("br"),r("span",{staticClass:"line-number"},[s._v("10")]),r("br"),r("span",{staticClass:"line-number"},[s._v("11")]),r("br"),r("span",{staticClass:"line-number"},[s._v("12")]),r("br"),r("span",{staticClass:"line-number"},[s._v("13")]),r("br"),r("span",{staticClass:"line-number"},[s._v("14")]),r("br"),r("span",{staticClass:"line-number"},[s._v("15")]),r("br"),r("span",{staticClass:"line-number"},[s._v("16")]),r("br"),r("span",{staticClass:"line-number"},[s._v("17")]),r("br"),r("span",{staticClass:"line-number"},[s._v("18")]),r("br"),r("span",{staticClass:"line-number"},[s._v("19")]),r("br")])])]),s._v(" "),r("li",[r("p",[s._v("运行爬虫")]),s._v(" "),r("ul",[r("li",[s._v("在爬虫服务器上，进入爬虫文件所在的路径，然后输入命令："),r("code",[s._v("scrapy runspider [爬虫名字]")])]),s._v(" "),r("li",[s._v("在Redis服务器上，推入一个开始的url链接："),r("code",[s._v("redis-cli> lpush [redis_key] start_url")]),s._v(" 开始爬取")])])])])])}),[],!1,null,null,null);a.default=t.exports}}]);